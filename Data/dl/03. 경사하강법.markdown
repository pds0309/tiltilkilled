

<br>


## 경사하강법(Gradient Descent) 알고리즘
 
<br>

손실함수(MSE, 크로스 엔트로피 등)가 찾아야되는 최적의 `모델 파라미터`를 찾는 최적의 알고리즘을 말함

학습률과 손실함수의 순간 기울기(1차 미분계수)를 이용하여 오차를 최소화 함

기울기를 구하며 새로운 가중치(w) 를 찾아 나가는 것

<br>

![경사1](https://user-images.githubusercontent.com/76927397/170611299-19074d4c-040a-4822-99fe-1364786519bf.JPG)

<br>


<br>

### 전역 최소 vs 지역 최소

<br>

![image](https://user-images.githubusercontent.com/76927397/170616874-08024883-af6a-414b-9531-f24e9a19390d.png)

<br>

이론적으로는 전역최소 기울기를 찾는 것이 맞지만 비용이 크거나 불가능할 수 있다.

대부분의 경사하강법 알고리즘은 지역최소 수치를 찾음


<br>


### 경사하강법을 신경망에 적용

<br>

![rt4](https://user-images.githubusercontent.com/76927397/170619130-fcd18a0c-642b-409d-8100-fcbc4e5a6bd7.JPG)


<br>

결국 인공신경망을 학습한다는 것은 	 weight 를 변경시켜주는 것인데

이 과정에서 경사하강법을 통해 얻은 new weight 를 활용해 모델을 개선해 나가는 것



<br>

### 단점

<br>

![rt5](https://user-images.githubusercontent.com/76927397/170620007-3094a03b-3420-4bee-b1fc-27c084559bcb.JPG)


<br>

매 가중치마다 동일한 미분 연산이 중복 처리되기 때문에 비효율적일 수 있다.

<br>


### 오차 역전파 알고리즘

출력값에 대한 입력값의 미분값을 출력층에서부터 계산해 거꾸로 전파하는 것

처음 가중치에서 구했던 중복 미분코드(오차)를 다음 가중치에 적용할 때 재사용하는 것

파라미터 수가 많아져도 효율적으로 신경망 모델을 학습시키는게 가능해짐


<br>

**문제점**


Gradient Vanishing(손실) 발생

> 신경망의 활성함수에 값이 계속 곱해지다가 결과 기울기가 0이 되어버리는 것

<br>

Gradient Exlpoding(폭주) 발생

> 기울기가 입력층으로 갈수록 점차 커지다가 가중치들이 비정상적으로 큰 값이 되며 발산하는 현상












