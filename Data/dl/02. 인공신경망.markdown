

<br>


## 인공신경망 겉핥기

<br>



### 퍼셉트론 기본 개념

- 다수의 입력을 받아서 처리한 후에 하나의 신호를 출력한다는 알고리즘

<br>

![image](https://user-images.githubusercontent.com/76927397/170394483-a83f1231-7e71-49be-ac89-67e4346f0e75.png)


<br>


- 인간이 느끼는 신경 구조와 비슷하다고 해서 인공 신경이라고 한다.

<br>

<br>

### 퍼셉트론 수식

- `가중치w` `bias` `입력데이터`가 가중화된 것

- 선형회귀식과 매우 유사한 구조

- `bias` 는 하나의 뉴런으로 입력된 가중합에 더해지는 상수로 출력값 조절 역할을 한다.

<br>

`w1x1 + w2x2 + b`

![퍼셉트론1](https://user-images.githubusercontent.com/76927397/170395108-accc08c1-00dd-47cf-9e9d-a224bb014513.JPG)


<br>

**f = 활성함수**

- 입력 신호의 총합을 출력 신호로 변환하는 함수

- 입력받은 신호를 얼마나 출력할지 결정하고 네트워크 층을 쌓아 비선형성을 표현

- `계단함수` , `시그모이드`, `RELU` 등의 함수를 이용해 출력신호로 변환


<br>

<br>


### 퍼셉트론 종류

<br>

**Single-Layer(단층) 퍼셉트론**

- 입력층에서 출력층으로 하나의 층이 존재


![image](https://user-images.githubusercontent.com/76927397/170397087-51dc75a3-0f5f-4b77-b7f0-354f340eee4f.png)

<br>

- 단순 회귀선 형태로 단층 퍼셉트론으로 `단순분류` 가 가능함.

- `XOR` 문제를 해결할 수 없다.

<br>

> 단순 회귀선 하나만 존재하기 때문에 다음과 같은 상황에서 분류가 불가능

![image](https://user-images.githubusercontent.com/76927397/170398518-419b33b7-8019-4521-a163-aa7de74f64ff.png)


<br>


<br>

**MLP(Multi-Layer) 퍼셉트론**

- 입력층과 출력층 사이에 `은닉층` 존재

- 한 레이어의 모든 뉴런이 다음 레이어의 모든 뉴런과 연결되어 `완전연결층` 이라고도 함

<br>

![퍼셉3](https://user-images.githubusercontent.com/76927397/170396693-b531db43-23c2-4c27-aafb-f3c7de5ba73e.JPG)

<br>

![퍼셉4](https://user-images.githubusercontent.com/76927397/170399007-9eac5aea-73b9-4132-8974-21e0eb380581.JPG)


- 요약하면 추가된 은닉층을 활용하여 선을 하나 추가로 그리는 효과를 얻을 수 있는 것

- 이론적으로는 여러개의 퍼셉트론을 활용하면 복잡한 문제도 해결할 수 있다





<br>

**Deep Neural Network(심층 신경망)**

- 은닉층이 2개 이상인 다층 퍼셉트론

<br>


**메모**

- 은닉층이 많을 때 성능 향상을 기대할 수 있어 과거에는 은닉층을 쌓는 것(모델) 에 중점을 두었음

- 현재는 모델보다 데이터의 깔끔함(클렌징 정도)에도 중점을 둠

- 레이어(은닉층)가 많아지면 많은 행렬 연산이 발생한다. => GPU 성능 필요


<br>


<br>

### 신경망 모델의 크기


**너비**

- 레이어별 뉴런 수

- 이론적으로는 입력에 대한 피쳐(특징)가 많고 복잡할 경우 이런 특징들을 학습할 뉴런 수를 늘린다.



**깊이**

- 레이어 수

- 특징의 `추상화` 작업이 필요할 수록 레이어 숫자를 늘려야 한다.

> 이미지에 대한 처리라고 가정해보았을 때 현실의 고양이 이미지는 매우 다양하고 복잡한 구조이다. 이런 것들을 피카소 그림마냥 특징을 추려내어 일반화하는 것을 말함 



<br>




<br>

<br>


### 신경망 요약

<br>


![퍼셉5](https://user-images.githubusercontent.com/76927397/170406167-f5d0d984-8ce7-41aa-87e5-f34738a7a92f.JPG)


<br>

**신경망 학습의 의미**

- **결국 최적의 bias와 최적의 가중치(`Model Parameter`)를 찾는 것**

- 영향이 적은피쳐 적은 가중치, 영향이 큰 피쳐는 큰 가중치를

<br>


**가중합 구하기**

- 피쳐에서 중요한 영향을 미치는 데이터를 선택하는 과정

- `z = w1x1 + w2x2 ... + b0` 와 같은 선형식

<br>

**레이어(은닉층) 의 역할**

- 가중합 + 활성화 함수

- 데이터의 특징을 추출해주는 것

- 같은 레이어(하나의 은닉층)에 존재하는  뉴런마다 서로 다른 특징을 추출함

<br>

**활성함수**

- 중요한 특징을 가지고 있는 피쳐들을 가중치 처리해서 얻은 가중합 선형 식을 **비선형으로 만드는 과정**

- 선형식만으로는 데이터 분류가 어려움


<br>

**모델 파라미터 최적화**


최적 알고리즘을 사용한다. (경사하강법)
  
`회귀` 에서는 잔차(실제값-예측값)의 최소를 목표로 모델 파라미터를 찾는다.
 
`분류` 에서는 관측확률 분포와 예측확률분포의 차이를 최소화 하는 것을 목표로 모델 파라미터를 찾는다.



 
